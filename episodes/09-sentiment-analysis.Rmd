---
title: 'Text mining, sentiment analysis, and visualization'
date: 'created on 22 November 2020 and updated `r format(Sys.time(), "%d %B, %Y")`'
output:
  unilur::tutorial_html_solution:
    toc: true
    toc_float: true
    toc_depth: 3
    suffix: ""
    theme: journal
    highlight: kate
    number_sections: no
    number_subsections: no
---

```{r knitr_init, echo=FALSE, cache=FALSE, include=FALSE}
library(knitr)
library(unilur)

## Global options
options(max.print="90")
opts_chunk$set(echo=TRUE,
               cache=FALSE, #TRUE
               prompt=FALSE,
               tidy=FALSE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=90)
options(width = 90)

# boxes custom
#remotes::install_github("koncina/unilur")

knitr::opts_template$set(alert = list(box.title = "Watch out!",
                                      box.body = list(fill = "#ffe0d9", colour = "black"),
                                      box.header = list(fill = "#FFAD99", colour = "black"),
                                      box.collapse = NULL,
                                      box.icon = "fa-exclamation-triangle"))
knitr::opts_template$set(solution = list(box.title = "Solution",
                                         box.body = list(fill = "#e6f6e7", colour = "black"),
                                         box.header = list(fill = "#ace1af", colour = "black"),
                                         box.icon = "fa-check-square",
                                         box.collapse = TRUE))
knitr::opts_template$set(objectives = list(box.title = "Objectives",
                                            box.body = list(fill = "#bbe8f4", colour = "black"),
                                            box.header = list(fill = "#64c9e6", colour = "black"),
                                            box.icon = "fa-info-circle",
                                            box.collapse = NULL))
knitr::opts_template$set(questions = list(box.title = "Questions",
                                      box.body = list(fill = "#fff9dc", colour = "black"),
                                      box.header = list(fill = "#ffec8b", colour = "black"),
                                      box.icon = "fa-search",
                                      box.collapse = NULL))
knitr::opts_template$set(information = list(box.title = "Keypoints",
                                       box.body = list(fill = "#fff9dc", colour = "black"),
                                      box.header = list(fill = "#ffec8b", colour = "black"),
                                      box.icon = "fa-search",
                                      box.collapse = NULL))

```

```{block, opts.label = "objectives" }
- try out the basics of text analysis
- load digital-born PDF data into R
- learn what the concepts of tokenisation, word frequency, and wordclouds mean
- learn about sentiment dictionaries 
- explore and evaluate different approaches to sentiment measurement
- apply the toolkit to a digital text of your choice (English or Danish) 

```


This episode is intended for intermediate learners of R who wish to explore Sentiment Analysis. You can follow and execute individual chunks in this `rmarkdown` document and analyze the emotional loading of the IPCC (International Panel on Climate Change) Special Report on Climate Change. Once you understand the digital workflow you can analyze digital text of your choice. Ask questions whenever chunks do not render or produce confusing outputs.

We start first with IPCC text loading and data wrangling, and introduce basic text-mining concepts. Then we spend the bulk of time demonstrating different kinds of sentiment measurement with R tools (`tidytext`). We visualize the results in order to assess the strengths and shortcomings of these approaches for different research tasks. 

A fantastic resource on tools and concepts is Julia Silge and David Robinson's [Text Mining with R](https://www.tidytextmining.com/).

Another text to accompany and further explain these concepts is by Nina Tahmasebi and Simon Hengchen (2019) [The Strengths and Pitfalls of Large-Scale Text Mining for Literary studies, Samlaren](http://uu.diva-portal.org/smash/get/diva2:1415010/FULLTEXT01.pdf)

### Get your environment set up
```{r}
# Load general libraries
library(tidyverse)
library(here)

# Load libraries for text mining:
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)
```


```{block, box.title = "Note", box.body = list(fill = "white"), box.icon = "fa-star"}
For more text analysis code, you can fork & work through Casey O’Hara and Jessica Couture’s eco-data-sci workshop (available here https://github.com/oharac/text_workshop)
```


### Get the IPCC report into R

```{r get-document}
ipcc_path <- here("data","ipcc_gw_15.pdf")
ipcc_text <- pdf_text(ipcc_path)
```

Some things to notice:

- How cool to extract text out of a PDF! Do you think it will work with any PDF?
- Each row is a page of the PDF (i.e., this is a vector of strings, one for each page)
- The pdf_text() function only sees text that is "selectable"

Example: Just want to get text from a single page (e.g. Page 9)? 
```{r single-page}
ipcc_p9 <- ipcc_text[9]
ipcc_p9
```

See how that compares to the text in the PDF on Page 9. What has `pdftools` library added and where?

```{block, box.title = "Note", box.body = list(fill = "white"), box.icon = "fa-star"}
From Jessica and Casey's text mining workshop: “pdf_text() returns a vector of strings, one for each page of the pdf. So we can mess with it in tidyverse style, let’s turn it into a dataframe, and keep track of the pages. Then we can use stringr::str_split() to break the pages up into individual lines. Each line of the pdf is concluded with a backslash-n, so split on this. We will also add a line number in addition to the page number."
```


### Wrangle the report in shape for analysis:

- Split up pages into separate lines (separated by `\n`) using `stringr::str_split()`
- Unnest into regular columns using `tidyr::unnest()`
- Remove leading/trailing white space with `stringr::str_trim()`

```{r split-lines}
ipcc_df <- data.frame(ipcc_text) %>% 
  mutate(text_full = str_split(ipcc_text, pattern = '\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 

# Why '\\n' instead of '\n'? Because some symbols (e.g. \, *) need to be called literally with a starting \ to escape the regular expression. For example, \\a for a string actually contains \a. So the string that represents the regular expression '\n' is actually '\\n'.
# Although, this time round, it is working for me with \n alone. Wonders never cease.

# More information: https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html

```

Now each line, on each page, is its own row, with extra starting & trailing spaces removed. 

### Get the tokens (individual words) in tidy format

Use `tidytext::unnest_tokens()` (which pulls from the `tokenizer`) package, to split columns into tokens. We are interested in *words*, so that's the token we'll use:

```{r tokenize}
ipcc_tokens <- ipcc_df %>% 
  unnest_tokens(word, text_full)
ipcc_tokens

# See how this differs from `ipcc_df`
# Each word has its own row!
```

Let's count the words!
```{r count-words}
ipcc_wc <- ipcc_tokens %>% 
  count(word) %>% 
  arrange(-n)
ipcc_wc
```

OK...so we notice that a whole bunch of things show up frequently that we might not be interested in ("a", "the", "and", etc.). These are called *stop words*. Let's remove them. 

### Remove stop words:

See `?stop_words` and `View(stop_words)`to look at documentation for stop words lexicons.

We will *remove* stop words using `tidyr::anti_join()`:
```{r stopwords}
ipcc_stop <- ipcc_tokens %>% 
  anti_join(stop_words) %>% 
  select(-ipcc_text)
```

Now check the counts again: 
```{r count-words2}
ipcc_swc <- ipcc_stop %>% 
  count(word) %>% 
  arrange(-n)
```

What if we want to get rid of all the numbers (non-text) in `ipcc_stop`?
```{r skip-numbers}
# This code will filter out numbers by asking:
# If you convert to as.numeric, is it NA (meaning those words)?
# If it IS NA (is.na), then keep it (so all words are kept)
# Anything that is converted to a number is removed

ipcc_no_numeric <- ipcc_stop %>% 
  filter(is.na(as.numeric(word)))
```

### A word cloud of IPCC report words (non-numeric)

See more: https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html

```{r wordcloud-prep}
# There are almost 2000 unique words 
length(unique(ipcc_no_numeric$word))

# We probably don't want to include them all in a word cloud. Let's filter to only include the top 100 most frequent?
ipcc_top100 <- ipcc_no_numeric %>% 
  count(word) %>% 
  arrange(-n) %>% 
  head(100)
```

```{r wordcloud}
ipcc_cloud <- ggplot(data = ipcc_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

ipcc_cloud
```

That's underwhelming. Let's customize it a bit:
```{r wordcloud-pro}
ggplot(data = ipcc_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("darkgreen","blue","red")) +
  theme_minimal()
```

Cool! And you can facet wrap (for different reports, for example) and update other aesthetics. See more here: https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html

### Sentiment analysis

First, check out the ‘sentiments’ lexicon. Julia Silge and David Robinson in their [book](https://www.tidytextmining.com/sentiment.html) say that:

"The three general-purpose lexicons are

  -  AFINN from Finn Årup Nielsen,
  -  bing from Bing Liu and collaborators, and
  -  nrc from Saif Mohammad and Peter Turney

All three of these lexicons are based on unigrams, i.e., single words. These lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth. The `AFINN` lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.  The `bing` lexicon categorizes words in a binary fashion into positive and negative categories. The `nrc` lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.  All of this information is tabulated in the sentiments dataset, and tidytext provides a function `get_sentiments()` to get specific sentiment lexicons without the columns that are not used in that lexicon."

Let's explore the sentiment lexicons. "bing" is included in the `tidytext` library, other lexicons ("afinn", "nrc", "loughran") you'll be prompted to download the first time you use them.

```{r, opts.label = "alert"}
# Attach tidytext and textdata packages

# Uncomment the line below the first time you install the nrc dictionary 
# get_sentiments(lexicon = "nrc")
# When you get prompted to install lexicon - choose yes!


# Uncomment the line below the first time you install the afinn dictionary
# get_sentiments(lexicon = "afinn")
# When you get prompted to install lexicon - choose yes!
```

```{block,  box.title = "Note", box.body = list(fill = "white"), box.icon = "fa-star"}
**WARNING:** These collections include very offensive words. It's best not to look at them in class.
```


`afinn`: Words ranked from -5 (very negative) to +5 (very positive)
http://corpustext.com/reference/sentiment_afinn.html

```{r afinn}
get_sentiments(lexicon = "afinn")
# Note: may be prompted to download (yes)

# Let's look at the pretty positive words:
afinn_pos <- get_sentiments("afinn") %>% 
  filter(value %in% c(3,4,5))

# Do not look at negative words in class. 
afinn_pos
```

`bing`: binary, "positive" or "negative" words.
https://search.r-project.org/CRAN/refmans/textdata/html/lexicon_bing.html
```{r bing}
get_sentiments(lexicon = "bing")
```

`nrc`:  Includes bins for 8 emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) and positive / negative. 

https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm

```{r nrc}
get_sentiments(lexicon = "nrc")
```


```{block, box.title = "Note", box.body = list(fill = "#fff9dc"), box.icon = "fa-star"}

**Citations for all the lexicons**


Crowdsourcing a Word-Emotion Association Lexicon, Saif Mohammad and Peter Turney, Computational Intelligence, 29 (3), 436-465, 2013.

Finn Årup Nielsen A new ANEW: Evaluation of a word list for sentiment analysis in microblogs. Proceedings of the ESWC2011 Workshop on 'Making Sense of Microposts': Big things come in small packages 718 in CEUR Workshop Proceedings 93-98. 2011 May. http://arxiv.org/abs/1103.2903.

Minqing Hu and Bing Liu, “Mining and summarizing customer reviews.”, Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD-2004), 2004.
```

Let's do sentiment analysis on the IPCC text data using the `afinn` and `nrc` lexicons. 


#### Sentiment analysis with afinn 

First, bind words in `ipcc_stop` to `afinn` lexicon:
```{r bind-afinn}
ipcc_afinn <- ipcc_stop %>% 
  inner_join(get_sentiments("afinn"))
```

Let's find some counts (by sentiment ranking):
```{r count-afinn}
ipcc_afinn_hist <- ipcc_afinn %>% 
  count(value)

# Plot them: 
ggplot(data = ipcc_afinn_hist, aes(x = value, y = n)) +
  geom_col() +
  theme_bw()
```

Investigate some of the words in a bit more depth:
```{r afinn-2}
# What are these '2' words?
ipcc_afinn2 <- ipcc_afinn %>% 
  filter(value == 2)
```

```{r afinn-2-more}
# Check the unique 2-score words:
unique(ipcc_afinn2$word)

# Count & plot them
ipcc_afinn2_n <- ipcc_afinn2 %>% 
  count(word, sort = TRUE) %>% 
  mutate(word = fct_reorder(factor(word), n))


ggplot(data = ipcc_afinn2_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip() +
  theme_bw()

# OK so what's the deal with confidence? And is it really "positive" in the emotion sense? 
```

Look back at the IPCC report, and search for "confidence." Is it typically associated with emotion, or something else? 

We learn something important from this example: Just using a sentiment lexicon to match words will not differentiate between different uses of the word...(ML can start figuring it out with context, but we won't do that here).

Or we can summarize sentiment for the report: 
```{r summarize-afinn}
ipcc_summary <- ipcc_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
```

The mean and median indicate *slightly* positive overall sentiments based on the AFINN lexicon. 

#### Sentiment analysis with nrc

We can use the `nrc` lexicon to start "binning" text by the feelings they're typically associated with. As above, we'll use `inner_join()` to combine the IPCC non-stopword text with the `nrc` lexicon: 

```{r bind-nrc}
ipcc_nrc <- ipcc_stop %>% 
  inner_join(get_sentiments("nrc"))
```

Wait, won't that exclude some of the words in our text? YES! We should check which are excluded using `anti_join()`:

```{r check-exclusions}
ipcc_exclude <- ipcc_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(ipcc_exclude)

# Count to find the most excluded:
ipcc_exclude_n <- ipcc_exclude %>% 
  count(word, sort = TRUE)

head(ipcc_exclude_n)
```

**Lesson: always check which words are EXCLUDED in sentiment analysis using a pre-built lexicon! **

Now find some counts: 
```{r count-nrc}
ipcc_nrc_n <- ipcc_nrc %>% 
  count(sentiment, sort = TRUE)

# And plot them:

ggplot(data = ipcc_nrc_n, aes(x = sentiment, y = n)) +
  geom_col()+
  theme_bw()
```

Or count by sentiment *and* word, then facet:
```{r count-nrc-facet}
ipcc_nrc_n5 <- ipcc_nrc %>% 
  count(word,sentiment, sort = TRUE) %>% 
  group_by(sentiment) %>% 
  top_n(5) %>% 
  ungroup()

ipcc_nrc_gg <- ggplot(data = ipcc_nrc_n5, aes(x = reorder(word,n), y = n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 2, scales = "free") +
  coord_flip() +
  theme_minimal() +
  labs(x = "Word", y = "count")

# Show it
ipcc_nrc_gg

# Save it
ggsave(plot = ipcc_nrc_gg, 
       here("figures","ipcc_nrc_sentiment.png"), 
       height = 8, 
       width = 5)

```

Wait, so "confidence" is showing up in NRC lexicon as "fear"? Let's check:
```{r nrc-confidence}
conf <- get_sentiments(lexicon = "nrc") %>% 
  filter(word == "confidence")

# Yep, check it out:
conf
```

### Big picture takeaway

There are serious limitations of sentiment analysis depending on what existing lexicons you use. You should **think really hard** about your findings and if a lexicon makes sense for your study. Otherwise, word counts and exploration alone can be useful! 

### Your task

Choose one of the tasks below to practice your newly acquired sentiment analysis skills:

1. Taking this script as a point of departure, apply sentiment analysis on the Game of Thrones. You will find a GOT.pdf in the data folder. What are the most common meaningful words and what emotions do you expect will dominate this volume? Are there any terms that are similarly ambiguous to the 'confidence' above? 

2. Choose an English text of your own and subject it to sentiment analysis. For example, you can use the Arabian Nights from lesson 08-text-analysis.Rmd

3. Choose a Danish text of your preference and analyze it. Beware that for each language you need an appropriate sentiment dictionary. For Danish there is the 'sentida' package, available at [https://github.com/Guscode/Sentida](https://github.com/Guscode/Sentida). The downloading instructions are available in the Readme - ask your instructors for clarification.

### Credits 
This tutorial was inspired by Allison Horst's Advanced Statistics and Data Analysis.

```{block, opts.label = "information"}

- `tidytext` is a toolbox packed with text-mining functions for structuring and analyzing text
- `pdftools` library can parse a well-formed digitally-born PDF document
- a number of different sentiment lexicons are freely available 
- success of sentiment analysis hinges on the suitability of sentiment lexicons for the research topic at hand
``` 

## Session info
```{r}
sessionInfo()
```